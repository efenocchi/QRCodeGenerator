{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7de9425-b06c-4948-aa88-81b8c401ce74",
      "metadata": {
        "id": "a7de9425-b06c-4948-aa88-81b8c401ce74"
      },
      "source": [
        "# Creating Artistic QR Codes at Scale Using LangChain and ControlNet\n",
        "\n",
        "## Summary\n",
        "We built a tool that can generate artistic QR codes for a specific website/url with the use of [Deep Lake](https://www.activeloop.ai/), [LangChain](https://python.langchain.com/docs/get_started/introduction.html), [Stable Diffusion](https://www.activeloop.ai/resources/glossary/stable-diffusion/) and [ControlNet](https://github.com/Mikubill/sd-webui-controlnet) via [AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) and [ComfyUI](https://github.com/comfyanonymous/ComfyUI). If you want to try the code directly from our notebook, just download it from the [repository](https://github.com/efenocchi/QRCodeGenerator/blob/main/QRCode_article.ipynb).\n",
        "\n",
        "Deep Lake is a Database for AI, designed to efficiently store and search large-scale AI data including audio, video or embeddings from text documents, which will also be utilized in this article. It offers unique storage optimization for deep learning applications, featuring data streaming, vector search, data versioning, and seamless integration with other popular frameworks such as LangChain. This comprehensive toolkit is designed to simplify the process of developing workflows of large language model (LLM), and in our case we will focus on its capability to summarize and answer questions from large-scale documents such as web pages.\n",
        "\n",
        "Stable diffusion is a recent development in the field of image synthesis, with exciting potential for reducing high computational demand. It is primarily used for text-to-image generation, but is capable of variety of other tasks such as image modification, inpainting, outpainting, upscaling and generating image-to-image conditioned on text input. Meanwhile, ControlNet is an innovative neural network architecture that is a game-changer in managing the control of these diffusion models by integrating extra conditions. These control techniques include edge and line detection, human poses, image segmentation, depth maps, image styles or simple user scribbles. By applying these techniques, it is then possible to condition our output image with QR codes as well. In case you would be interested in more details, we recommend reading the [original ControlNet article](https://arxiv.org/abs/2302.05543).\n",
        "\n",
        "\n",
        "By combining all of this, we can achieve a scalable generation of QR codes that are very unique and more likely will attract attention. Overall, there are many possibilities you can approach this problem, and in this article, we will present those that we believe have the highest potential to impact advertising in the future. These are the steps that we are going to walk you through:\n",
        "\n",
        "## Steps\n",
        "1. Scraping the Content From a Website and Splitting It Into Documents\n",
        "2. Saving the Documents Along With Their Embeddings to Deep Lake\n",
        "3. Extracting the Most Relevant Documents\n",
        "4. Creating Prompts to Generate an Image Based on Documents\n",
        "    - 4.1 Custom summary prompt + LLMChain\n",
        "    - 4.2 QA retrieval + LLM\n",
        "5. Summarizing the Created Prompts\n",
        "6. Generating Simple QR From URL and inserting custom logo\n",
        "7. Generating Artistic QR Codes for Activeloop\n",
        "    - 7.1. Txt2Img\n",
        "        - 7.1.1 Content prompt\n",
        "        - 7.1.2 Portrait prompt\n",
        "        - 7.1.3 Deep Lake prompt\n",
        "    - 7.2. Img2Img with logo\n",
        "        - 7.2.1 Content prompt\n",
        "        - 7.2.2 Portrait prompt\n",
        "        - 7.2.3 Deep Lake prompt\n",
        "8. Generating Artistic QR Codes for E-commerce\n",
        "    - 8.1. Img2Img with logo - Tommy Hilfiger\n",
        "    - 8.2. Img2Img with logo - Patagonia\n",
        "9. Hands on with ComfyUI\n",
        "10. Limitations of Our Approach\n",
        "11. Conclusion\n",
        "12. FAQs      \n",
        "\n",
        "Before we start, we need to install requirements, import LangChain and set the following API tokens:\n",
        "- Apify token - web scraping/crawling\n",
        "- Activeloop token - Deep Lake Vector Store\n",
        "- OpenAI token - language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7w-_U2M3f1o",
      "metadata": {
        "id": "V7w-_U2M3f1o"
      },
      "outputs": [],
      "source": [
        "!pip install langchain deeplake openai qrcode apify_client tiktoken langchain-openai\n",
        "!apt install libzbar0\n",
        "!pip install qreader opencv-python python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6789b8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69054140-e6d1-4b17-82b2-85c1c549f8cd",
      "metadata": {
        "id": "69054140-e6d1-4b17-82b2-85c1c549f8cd"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "## pip install openai langchain deeplake apify-client tiktoken pydantic==1.10.8\n",
        "\n",
        "# Import libraries\n",
        "from langchain_community.vectorstores import DeepLake\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.utilities import ApifyWrapper\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#from langchain.document_loaders.base import Document\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Set API tokens\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ['ACTIVELOOP_TOKEN'] = os.getenv('ACTIVELOOP_TOKEN')\n",
        "os.environ[\"APIFY_API_TOKEN\"] = os.getenv('APIFY_API_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49005f18-3758-4188-a8b3-6024513c2b77",
      "metadata": {
        "id": "49005f18-3758-4188-a8b3-6024513c2b77"
      },
      "source": [
        "### Step 1: Scraping the Content From a Website and Splitting It Into Documents\n",
        "First of all, we need to collect data that will be used as a content used to generate QR codes. Since the goal is to personalize it to a specific website, we provide a simple pipeline that can crawl data from a given URL. As an example, we use https://www.activeloop.ai/ from which we scraped 20 pages, but you could use any other website as long as it does not violate the Terms of Use. Or, if you wish to use other type of content, LangChain provide many other [File loaders](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/) and [Website loaders](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/web_loaders/) and you can personalize QR codes for them too!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f825bf2-d1bc-4ada-a48d-2593931b5d5a",
      "metadata": {
        "id": "6f825bf2-d1bc-4ada-a48d-2593931b5d5a"
      },
      "outputs": [],
      "source": [
        "# We use crawler from ApifyWrapper(), which is available in Langchain\n",
        "# For convenience, we set 20 maximum pages to crawl with a timeout of 300 seconds.\n",
        "apify = ApifyWrapper()\n",
        "loader = apify.call_actor(\n",
        "    actor_id=\"apify/website-content-crawler\",\n",
        "    run_input={\"startUrls\": [{\"url\": \"https://www.activeloop.ai/\"}], \"maxCrawlPages\": 20},\n",
        "    dataset_mapping_function=lambda item: Document(\n",
        "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
        "    ),\n",
        "    timeout_secs=300,\n",
        ")\n",
        "\n",
        "# Now the pages are loaded and split into chunks with a maximum size of 1000 tokens\n",
        "pages = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator = \".\")\n",
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-95Aq6llDyNA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-95Aq6llDyNA",
        "outputId": "fce10dfe-769b-4e18-9766-a63e623fa2ec"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8768b20-ae0d-418e-b6c4-b0472209a0ab",
      "metadata": {
        "id": "f8768b20-ae0d-418e-b6c4-b0472209a0ab"
      },
      "source": [
        "\n",
        "### Step 2: Saving the Documents Along With Their Embeddings to Deep Lake\n",
        "Once the website is scraped and pages are split into documents, it's time to generate the embeddings and save them to the Deep Lake. This means that we can come back to our previously scraped data at any time and don't need to recalculate the embeddings again. To do that, you need to set your `ACTIVELOOP_ORGANIZATION_ID`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34562194-71c7-45e2-b17b-a8de17120bc6",
      "metadata": {
        "id": "34562194-71c7-45e2-b17b-a8de17120bc6"
      },
      "outputs": [],
      "source": [
        "activeloop_org = \"YOUR_ACTIVELOOP_ORG_ID\"\n",
        "# initialize the embedding model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# initialize the database, can also be used to load the database\n",
        "db = DeepLake(\n",
        "    dataset_path=f\"hub://{activeloop_org}/scraped-websites\",\n",
        "    embedding=embeddings,\n",
        "    overwrite=False,\n",
        ")\n",
        "\n",
        "# save the documents\n",
        "db.add_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a862b180-983f-420e-b747-1ff0c836aae1",
      "metadata": {
        "id": "a862b180-983f-420e-b747-1ff0c836aae1"
      },
      "source": [
        "### Step 3: Extracting the Most Relevant Documents\n",
        "Since we want to generate an image in the context of the given website that can have hundreds of pages, it is useful to filter documents that are the most relevant for our query, in order to save money on chained API calls to LLM. For this, we are going to leverage [Deep Lake Vector Store](https://docs.activeloop.ai/tutorials/vector-store/deep-lake-vector-store-in-langchain) similarity search as well as retrieval functionality.\n",
        "\n",
        "To pre-filter the documents based on a query, choose the query that works best for you based on the type of information you have scraped from the internet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fa9e2d-1ab1-43a4-80df-83aeab80cc8c",
      "metadata": {
        "id": "f9fa9e2d-1ab1-43a4-80df-83aeab80cc8c"
      },
      "outputs": [],
      "source": [
        "query_for_company = 'Business core of the company'\n",
        "\n",
        "result = db.similarity_search(query_for_company, k=10)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab2ccf1-34f7-4d82-a06e-3cff0bb2d381",
      "metadata": {
        "id": "7ab2ccf1-34f7-4d82-a06e-3cff0bb2d381"
      },
      "source": [
        "For question-answering pipeline, we can then define the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4f4402-94e4-4fbc-b7d0-391c71da06ba",
      "metadata": {
        "id": "da4f4402-94e4-4fbc-b7d0-391c71da06ba"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_kwargs={\"k\":10}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957d8966-3f2d-466e-b6a0-7fe649014523",
      "metadata": {
        "id": "957d8966-3f2d-466e-b6a0-7fe649014523"
      },
      "source": [
        "### Step 4: Creating Prompts to Generate an Image Based on Documents\n",
        "The goal is to understand the content and generate prompts in an automated way, so that the process can be scalable. We start by initializing the LLM with a default `gpt-3.5-turbo-instruct` model and set medium temperature to introduce some randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094a8527-2945-46d9-b346-a4deda173912",
      "metadata": {
        "id": "094a8527-2945-46d9-b346-a4deda173912"
      },
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = OpenAI(temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201e81e0-6154-4776-850f-e77653da7a3e",
      "metadata": {
        "id": "201e81e0-6154-4776-850f-e77653da7a3e"
      },
      "source": [
        "One of many advantages of LangChain are also prompt templates, which significantly help with clarity and readability. To make the output description more precise, we should also provide examples as can be seen here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e95428",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"You are a prompt generator. Based on the content, write a detailed one sentence description that can be used to generate an image\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebacb8b3-1019-4cbc-9cd3-0eec4028de79",
      "metadata": {
        "id": "ebacb8b3-1019-4cbc-9cd3-0eec4028de79"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"{query}:\n",
        "\n",
        "Content: {text}\n",
        "\"\"\"\n",
        "\n",
        "# set the prompt template\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"query\": query}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bca5b1-2c4c-4d47-a16c-b3fcbdd1090a",
      "metadata": {
        "id": "40bca5b1-2c4c-4d47-a16c-b3fcbdd1090a"
      },
      "source": [
        "The `query` is used to indicate some information we expect to output, `text` is the content provided to LLM, whereby it is supposed to provide a detailed description of the image. Additionally, to have more control over the output, we also create an alternative prompt that can generate a specific image type."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "444f7bdc-e69e-43c2-965a-75fec42fdda8",
      "metadata": {
        "id": "444f7bdc-e69e-43c2-965a-75fec42fdda8"
      },
      "source": [
        "Using this, we then experimented with 2 following approaches, that differ in what kind of `text` is provided.\n",
        "\n",
        "\n",
        "#### Option 1: Custom summary prompt with LLMChain\n",
        "\n",
        "The idea is simple, we chain the description prompt on each filtered document and then apply it once again on the summarized descriptions. In other words, `text` will be a variable that is iterated during `LMMChain` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f9d3ad-a5db-4c31-8b3b-53f9bc5cf2ff",
      "metadata": {
        "id": "65f9d3ad-a5db-4c31-8b3b-53f9bc5cf2ff"
      },
      "outputs": [],
      "source": [
        "# Initialize the chain\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
        "\n",
        "# Filter the most relevant documents\n",
        "result = db.similarity_search(query_for_company, k=10)\n",
        "# Run the Chain\n",
        "image_prompt = chain.invoke(result)\n",
        "image_prompt = image_prompt[\"text\"]\n",
        "image_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88f0309-5cc1-429e-9e1f-b3de6c4ed409",
      "metadata": {
        "id": "a88f0309-5cc1-429e-9e1f-b3de6c4ed409"
      },
      "source": [
        "#### Option 2: Retrieval Question-Answering with LLM\n",
        "\n",
        "Here we initialize QA retriever, which will allow us to ask to explain a particular concept on the filtered documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423fa62c-372d-4a30-ad55-b00bfeb4ef12",
      "metadata": {
        "id": "423fa62c-372d-4a30-ad55-b00bfeb4ef12"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "chain_answer = qa.invoke(\"Explain what is Deep Lake\")\n",
        "chain_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fa3227-ac47-47ae-a03e-483f80c36a9f",
      "metadata": {
        "id": "07fa3227-ac47-47ae-a03e-483f80c36a9f"
      },
      "source": [
        "The `answer` is then used as `text` in the `PromptTemplate` without the need for any chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b7a1b1-ca5a-486f-a5dc-53d37a4b80aa",
      "metadata": {
        "id": "b4b7a1b1-ca5a-486f-a5dc-53d37a4b80aa"
      },
      "outputs": [],
      "source": [
        "answer = llm(prompt=PROMPT.format(text=answer))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e281b59d-7814-4889-b724-22a7a07dc166",
      "metadata": {
        "id": "e281b59d-7814-4889-b724-22a7a07dc166"
      },
      "source": [
        "### Step 5: Summarizing the Created Prompts\n",
        "We experimented with different prompt setups in the previous section, and yet there is more to explore. In case you would be interested in perfectionizing your LLM prompts even further, we have an [amazing course](https://learn.activeloop.ai/courses/take/langchain/multimedia/46317727-intro-to-prompt-engineering-tips-and-tricks) that will provide you many useful tips and tricks. Mastering prompts for image generation is, however, more art than science. Nevertheless, by providing the LLM with examples we can see that it can do a pretty good job by generating very specific image descriptions. Here are 3 different types of prompts that we were able to generate with our approach:\n",
        "\n",
        "#### 1. Content prompt\n",
        "This prompt summarizes all relevant documents scraped from Activeloop into a general but detailed image description: `high-tech, futuristic, AI-driven, advanced, complex, computer-generated, robot, machine learning, data visualization, interactive, cutting-edge technology, automation, precision, efficiency, innovation, digital transformation, smart technology, science fiction-inspired.\n",
        "`\n",
        "\n",
        "#### 2. Portrait prompt\n",
        "Additionally to previous prompt, we also condition on the type of the image, which is in this case a detailed image description of a portrait: `High quality portrait of a developer working on LangChain, surrounded by computer screens and programming tools, with a focus on the keyboard and coding on the screen.\n",
        "`\n",
        "\n",
        "#### 3. Deep Lake prompt\n",
        "Here we show a Question-Answering example with a detailed image description of Deep Lake: `An aerial view of a serene, glassy lake surrounded by trees and mountains, with giant blocks of data floating on the surface, each block representing a different data type such as images, videos, audio, and tabular data, all stored as tensors, while a team of data scientists in a nearby cabin focus on their work to build advanced deep learning models, powered by GPUs that are seamlessly integrated with Deep Lake.\n",
        "`\n",
        "\n",
        "### Step 6: Generating Simple QR From URL and inserting custom logo\n",
        "Before we generate the art, it is important to prepare the simple QR code for ControlNet, which can be created directly from Python code. It is important to set the error correction level to 'H', which increases the probability of QR being readable, as 30% of the code can be covered/destroyed by an image. To generate a QR code with a logo, we created a function that takes the logo image and places it on the previously generated QR code. It is also important to note, that some of the URLs might be too long to generate a QR that is not too complicated and reliable enough for scanning. For this purpose, we can use url shorteners such as [bit.ly](bit.ly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1effbcac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import qrcode\n",
        "from PIL import Image\n",
        "\n",
        "def create_qrcode(url:str):\n",
        "    QRcode = qrcode.QRCode(\n",
        "        error_correction=qrcode.constants.ERROR_CORRECT_H\n",
        "    )\n",
        "\n",
        "    # taking url or text\n",
        "    url = 'https://www.activeloop.com/'\n",
        "\n",
        "    # adding URL or text to QRcode\n",
        "    QRcode.add_data(url)\n",
        "\n",
        "    # adding color to QR code\n",
        "    QRimg = QRcode.make_image(\n",
        "        back_color=\"white\").convert('RGB')\n",
        "    return QRimg\n",
        "\n",
        "def qr_with_logo(logo_path: str, QRimg: Image.Image, output_image_name: str):\n",
        "    logo = Image.open(logo_path)\n",
        "\n",
        "    # taking base width\n",
        "    basewidth = 100\n",
        "\n",
        "    # adjust image size\n",
        "    wpercent = (basewidth/float(logo.size[0]))\n",
        "    hsize = int((float(logo.size[1])*float(wpercent)))\n",
        "    logo = logo.resize((basewidth, hsize))\n",
        "\n",
        "    # set size of QR code\n",
        "    pos = ((QRimg.size[0] - logo.size[0]) // 2,\n",
        "        (QRimg.size[1] - logo.size[1]) // 2)\n",
        "    QRimg.paste(logo, pos)\n",
        "\n",
        "    # save the QR code generated\n",
        "    QRimg.save(output_image_name)\n",
        "    return QRimg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02af62d3",
      "metadata": {},
      "source": [
        "\n",
        "### Step 7: Generating Artistic QR Codes for Activeloop\n",
        "First of all, we need to keep in mind that it is still very fresh and unexplored topic and the more pleasing-looking QRs you want to generate, the higher risk of not being readable by a scanner. This results in an endless cycle of adjusting parameters to find the most general setup. Many approaches can be applied, but their main difference is in ControlNet units. The highest success we had was with [brightness and tile preprocessors](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main), as well as the [qrcode preprocessor](https://huggingface.co/DionTimmer/controlnet_qrcode). Sometimes, adding a depth preprocessor was also helpful. A great guide on how to set up the Stable-diffusion webui with ControlNet extension to generate your first QR codes can be found for example [here](https://www.youtube.com/watch?v=HOY5J9UT_lY). Nevertheless, there is no single setup that would work 100% of the time and a lot of experimenting is needed, especially in terms of finetuning the control's strength/start/end to achieve a desirable output.\n",
        "\n",
        "For example, in most of the QR codes we used the following setup:\n",
        "- Negative prompt: ugly, disfigured, low quality, blurry, nsfw\n",
        "- Steps: 20\n",
        "- Sampler: DPM++ 2M Karras\n",
        "- CFG scale: 9\n",
        "- Size: 768x768\n",
        "- Model: dreamshaper_631BakedVae\n",
        "- ControlNet\n",
        "    - 0: preprocessor: none, model: control_v1p_sd15_qrcode, weight: 1.1, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
        "    - 1: preprocessor: none, model: control_v1p_sd15_brightness, weight: 0.3, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
        "    \n",
        "In case of Img2Img, we would also need to put an inpaint mask to disable any changes to the logo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383c14bc",
      "metadata": {},
      "source": [
        "### Download and run AUTOMATIC1111\n",
        "In order to play with this interface you have to clone the [official repository](https://github.com/AUTOMATIC1111/stable-diffusion-webui) in you workspace and double click the `webui-user.bat` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3acfa04",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3aa28a",
      "metadata": {},
      "source": [
        "### Integrate ControlNet in AUTOMATIC1111 \n",
        "\n",
        "If you want to install ControlNet you must follow the istructions given in the [official repository](https://github.com/Mikubill/sd-webui-controlnet):\n",
        "\n",
        "- Open \"Extensions\" tab.\n",
        "- Open \"Install from URL\" tab in the tab.\n",
        "- Enter https://github.com/Mikubill/sd-webui-controlnet.git to \"URL for extension's git repository\".\n",
        "- Press \"Install\" button.\n",
        "- Wait for 5 seconds, and you will see the message \"Installed into stable-diffusion-webui\\extensions\\sd-webui-controlnet. Use Installed tab to restart\".\n",
        "- Go to \"Installed\" tab, click \"Check for updates\", and then click \"Apply and restart UI\". (The next time you can also use these buttons to update ControlNet.)\n",
        "- Completely restart A1111 webui including your terminal. (If you do not know what is a \"terminal\", you can reboot your computer to achieve the same effect.)\n",
        "- Download models (see below).\n",
        "- After you put models in the correct folder, you may need to refresh to see the models. The refresh button is right to your \"Model\" dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a64955",
      "metadata": {},
      "source": [
        "If you visit the official page of [AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui), you can see the interface from which you can select the settings you prefer.\n",
        "\n",
        "<img width=\"600\" src=\"https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc946cb",
      "metadata": {},
      "source": [
        "In the above part you can choose what type of task to perform `txt2img`, `img2img` and so on, in the middle part of the interface there are settings you can change. If you install the controlnet plugin you will be able to find all the configurations that will allow you to control how much the impact of ControlNet will weigh on the final result.\n",
        "\n",
        "<img width=\"830\" src=\"images/controlnet_interface_1.PNG\">\n",
        "<img width=\"800\" src=\"images/controlnet_interface_2.PNG\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34806367-7751-4fa7-9003-32eea8d9779d",
      "metadata": {
        "id": "34806367-7751-4fa7-9003-32eea8d9779d"
      },
      "source": [
        "#### Txt2Img - generating QR code from a simple QR and previously created prompt\n",
        "##### Content prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.1.1.1.png\"> | <img width=\"400\" src=\"images/7.1.1.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.1.1.3.png\"> | <img width=\"400\" src=\"images/7.1.1.4.png\">|\n",
        "\n",
        "##### Portrait prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.1.2.1.png\"> | <img width=\"400\" src=\"images/7.1.2.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.1.2.3.png\"> | <img width=\"400\" src=\"images/7.1.2.4.png\">|\n",
        "\n",
        "##### Deep Lake prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.1.3.1.png\"> | <img width=\"400\" src=\"images/7.1.3.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.1.3.3.png\"> | <img width=\"400\" src=\"images/7.1.3.4.png\">|\n",
        "        \n",
        "#### Img2Img with logo - generating QR code from a QR with logo and previously created prompt\n",
        "##### Content prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.2.1.1.png\"> | <img width=\"400\" src=\"images/7.2.1.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.2.1.3.png\"> | <img width=\"400\" src=\"images/7.2.1.4.png\">|\n",
        "\n",
        "##### Portrait prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.2.2.1.png\"> | <img width=\"400\" src=\"images/7.2.2.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.2.2.3.png\"> | <img width=\"400\" src=\"images/7.2.2.4.png\">|\n",
        "\n",
        "##### Deep Lake prompt\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "|<img width=\"400\" src=\"images/7.2.3.1.png\"> | <img width=\"400\" src=\"images/7.2.3.2.png\">|\n",
        "|<img width=\"400\" src=\"images/7.2.3.3.png\"> | <img width=\"400\" src=\"images/7.2.3.4.png\">|\n",
        "\n",
        "\n",
        "\n",
        "### Step 8: Generating Artistic QR Codes for E-commerce\n",
        "\n",
        "The idea here is a little different compared to the previous examples in context of [Activeloop](activeloop.com).\n",
        "Now, we focus on product advertising and we want to generate a QR code only for a single URL and its product. The challenge is to generate QR code, while also keeping the product as similar to the original as possible to avoid misleading information. To do this, we experimented with many preprocessors such as the `tile`, `depth`, `reference_only`, `lineart` or `styles`, but we found most of them too unreliable and far from being similar to the original input. At this moment, we believe that the most useful  is the `tile` preprocessor, which can preserve a lot of information. The disadvantage is, however, that it does not allow for many changes during control phase and the QR fit can sometimes be questionable. In practice, this would be done by adding another CotntrolNet unit:\n",
        "- 2: preprocessor: none, model: control_v11f1e_sd15_tile, weight: 1.0, starting/ending: (0, 1), resize mode: Crop and Resize, pixel perfect: False, control mode: Balanced\n",
        "Since the `tile` input image control is very strong, theres not much else we can do. Styles are one of the little extra adjustments possible and very useful style cheat sheet can be found [here](https://supagruen.github.io/StableDiffusion-CheatSheet/). For our purposes, however, we did not end up utilizing any of them.\n",
        "\n",
        "\n",
        "Similarly as before, we generated prompts automaticaly from the given websites. We randomly selected 2 products and in the first case (Tommy Hilfiger) We added logo to the initial basic QR code while in the second case (Patagonia), we only mask the logo that is already present on the product. To see the comparison, we also provide the original input images (Sources: [Patagonia](https://eu.patagonia.com/cz/en/product/mens-capilene-cool-daily-graphic-shirt/45235.html?dwvar_45235_color=SSMX&cgid=mens-shirts-tech-tops), [Tommy Hilfiger](https://uk.tommy.com/tommy-hilfiger-x-vacation-flag-embroidery-t-shirt-mw0mw33438ybl)).\n",
        "\n",
        "#### Img2Img with logo - generating Tommy Hilfiger QR code\n",
        "\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "| <img width=\"400\" src=\"images/8.1.1.png\"> | <img width=\"400\" src=\"images/8.1.2.png\"> |\n",
        "| <img width=\"400\" src=\"images/8.1.3.png\"> | <img width=\"400\" src=\"images/8.1.4.png\"> |\n",
        "\n",
        "#### Img2Img with logo - generating Patagonia QR code\n",
        "| | |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "| <img width=\"400\" src=\"images/8.2.1.png\"> | <img width=\"400\" src=\"images/8.2.2.png\"> |\n",
        "| <img width=\"400\" src=\"images/8.2.3.png\"> | <img width=\"400\" src=\"images/8.2.4.png\"> |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2348107",
      "metadata": {},
      "source": [
        "### Step 9: Hands on with ComfyUI\n",
        "A different approach from the one listed above can be obtained by taking advantage of ComfyUI which is a powerful and modular stable diffusion GUI.\n",
        "The main idea is to create a schema from the proposed GUI and transform this schema into code thanks to an extension called `ComfyUI-to-Python-Extension`.\n",
        "We need to load the Stable Diffusion and the ControlNet checkpoints we want to use. \n",
        "\n",
        "In our case we experimented with:\n",
        "- Diffusion models: `v1-5-pruned-emaonly`, `dreamshaper_8` and `revAnimated_v122EOL` \n",
        "- ControlNet models: `control_v1p_sd15_brightness`, `control_v1p_sd15_qrcode`, `control_v11f1e_sd15_tile` and `control_v11f1p_sd15_depth`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb34bc8",
      "metadata": {},
      "source": [
        "Install ComfyUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c135d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/comfyanonymous/ComfyUI.git\n",
        "%cd ComfyUI\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "559d1c84",
      "metadata": {},
      "source": [
        "Add the ComfyUI plugin to transform the schema into code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4775a91c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/pydn/ComfyUI-to-Python-Extension.git\n",
        "%cd ComfyUI-to-Python-Extension\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ececba1c",
      "metadata": {},
      "source": [
        "Download all the checkpoints in the right folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f77a4f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/efenocchi/QRCodeGenerator/blob/main/3_workflow_qr_codes2.json https://github.com/efenocchi/QRCodeGenerator/blob/main/2_workflow_qr_codes.json -P /content/ComfyUI/ComfyUI-to-Python-Extension\n",
        "!wget https://github.com/efenocchi/QRCodeGenerator/blob/main/workflow_api.py -P /content/ComfyUI/ComfyUI-to-Python-Extension\n",
        "!wget https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P /content/ComfyUI/models/vae\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth -P /content/ComfyUI/models/controlnet\n",
        "!wget https://huggingface.co/latentcat/latentcat-controlnet/resolve/main/models/control_v1p_sd15_brightness.safetensors -P /content/ComfyUI/models/controlnet\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1e_sd15_tile.pth -P /content/ComfyUI/models/controlnet\n",
        "!wget https://huggingface.co/autismanon/modeldump/resolve/main/dreamshaper_8.safetensors -P /content/ComfyUI/models/checkpoints\n",
        "!wget https://civitai.com/api/download/models/46846?type=Model&format=SafeTensor&size=full&fp=fp32 -P /content/ComfyUI/models/checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe3ff495",
      "metadata": {},
      "source": [
        "If you have problems downloading the model from CivitAI try downloading it manually after logging in or download it directly from Hugging Face, in this last case remember to pay attention to choosing the right name when loading the model in the following steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119b8278",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/emmajoanne/models/resolve/main/revAnimated_v122.safetensors -P /content/ComfyUI/models/checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ff19b1",
      "metadata": {},
      "source": [
        "Run the Comfyui GUI.\n",
        "\n",
        "If you encounter problems, run this command from the terminal. For advice and error resolution, refer to the [official repository](https://github.com/comfyanonymous/ComfyUI).\n",
        "\n",
        "Since this is not a ComfyUI guide and some things may not be clear, if you want to know more, consult the official repository or some free guides like [this one](https://www.youtube.com/watch?v=LNOlk8oz1nY&ab_channel=OlivioSarikas)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dbc0ecb",
      "metadata": {},
      "source": [
        "<span style=\"color:red\">This command has been put for informational purposes only, to proceed with image generation you will not need to create the scheme from scratch with ComfyUI but directly execute the cells shown below.</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c007767",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!python main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57f4e67",
      "metadata": {},
      "source": [
        "Below will be shown one of the schemes used, it is composed by 1 Diffusion model called `v1-5-pruned-emaonly` and 3 different controlnet models `control_v1p_sd15_brightness` and `control_v11f1e_sd15_tile`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046918b6",
      "metadata": {},
      "source": [
        "![schema_2_controlnet.webp](images/schema_2_controlnet.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc3d77b",
      "metadata": {},
      "source": [
        "As illustrated, the QR Code was generated by combining the basic QR Code image with a textual input, making it easy to merge the initial image with the generated one. In this case the positive prompt was simply \"a cyborg character\" and the negative one \"ugly, artefacts, bad\". This pipeline produced the images shown below:\n",
        "\n",
        "<img width=\"300\" src=\"images/qrcode_detail1.webp\"> <img width=\"300\" src=\"images/qrcode_detail2.webp\">\n",
        "\n",
        "<img width=\"300\" src=\"images/qrcode_detail3.webp\"> <img width=\"300\" src=\"images/qrcode_detail4.webp\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf94dc77",
      "metadata": {},
      "source": [
        "To export the schema and transform it to python code you must follow some different step:\n",
        "1) Enable Dev mode Options: you need to click on the settings button (located above to the Queue Prompt text in the window that will appear when you activate ComfyUI) and select the \"Enable Dev mode Options\" box.\n",
        "2) Export the schema via the button \"Save (API Format)\"\n",
        "3) Put this schema in the ComfyUI-to-Python-Extension folder\n",
        "4) Run the python file `comfyui_to_python.py`\n",
        "\n",
        "<span style=\"color:red\">As with the previous command, this explanation has also been made for informational purposes only, to proceed with image generation you will not need to create the scheme from scratch with ComfyUI and convert it into python code because this step has already been done by me.</span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8707b00",
      "metadata": {},
      "source": [
        "The following functions are used to create the QR code from a text and to load a logo in the center of it:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac02c9b",
      "metadata": {},
      "source": [
        "Go to the main ComfyUI folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3612ee2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if you are in Colab you can simply run\n",
        "# % cd /content/ComfyUI\n",
        "%cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb79a7ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "img = create_qrcode('https://www.activeloop.com/')\n",
        "img.save(\"activeloop_qr.jpg\")\n",
        "img_with_logo = qr_with_logo(\"activeloop_logo.jpg\", img, \"activeloop_qr_with_logo.jpg\")\n",
        "img_with_logo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40e3a61",
      "metadata": {},
      "source": [
        "Set the correct path to be able to work with ComfyUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vuvc0GdpUkQ6",
      "metadata": {
        "id": "vuvc0GdpUkQ6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "from typing import Sequence, Mapping, Any, Union\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_value_at_index(obj: Union[Sequence, Mapping], index: int) -> Any:\n",
        "    \"\"\"Returns the value at the given index of a sequence or mapping.\n",
        "\n",
        "    If the object is a sequence (like list or string), returns the value at the given index.\n",
        "    If the object is a mapping (like a dictionary), returns the value at the index-th key.\n",
        "\n",
        "    Some return a dictionary, in these cases, we look for the \"results\" key\n",
        "\n",
        "    Args:\n",
        "        obj (Union[Sequence, Mapping]): The object to retrieve the value from.\n",
        "        index (int): The index of the value to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        Any: The value at the given index.\n",
        "\n",
        "    Raises:\n",
        "        IndexError: If the index is out of bounds for the object and the object is not a mapping.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return obj[index]\n",
        "    except KeyError:\n",
        "        return obj[\"result\"][index]\n",
        "\n",
        "\n",
        "def find_path(name: str, path: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Recursively looks at parent folders starting from the given path until it finds the given name.\n",
        "    Returns the path as a Path object if found, or None otherwise.\n",
        "    \"\"\"\n",
        "    # If no path is given, use the current working directory\n",
        "    if path is None:\n",
        "        path = os.getcwd()\n",
        "\n",
        "    # Check if the current directory contains the name\n",
        "    if name in os.listdir(path):\n",
        "        path_name = os.path.join(path, name)\n",
        "        print(f\"{name} found: {path_name}\")\n",
        "        return path_name\n",
        "\n",
        "    # Get the parent directory\n",
        "    parent_directory = os.path.dirname(path)\n",
        "\n",
        "    # If the parent directory is the same as the current directory, we've reached the root and stop the search\n",
        "    if parent_directory == path:\n",
        "        return None\n",
        "\n",
        "    # Recursively call the function with the parent directory\n",
        "    return find_path(name, parent_directory)\n",
        "\n",
        "\n",
        "def add_comfyui_directory_to_sys_path() -> None:\n",
        "    \"\"\"\n",
        "    Add 'ComfyUI' to the sys.path\n",
        "    \"\"\"\n",
        "    comfyui_path = find_path(\"ComfyUI\")\n",
        "    if comfyui_path is not None and os.path.isdir(comfyui_path):\n",
        "        sys.path.append(comfyui_path)\n",
        "        print(f\"'{comfyui_path}' added to sys.path\")\n",
        "\n",
        "\n",
        "# def add_extra_model_paths() -> None:\n",
        "#     \"\"\"\n",
        "#     Parse the optional extra_model_paths.yaml file and add the parsed paths to the sys.path.\n",
        "#     \"\"\"\n",
        "#     from main import load_extra_path_config\n",
        "\n",
        "#     extra_model_paths = find_path(\"extra_model_paths.yaml\")\n",
        "\n",
        "#     if extra_model_paths is not None:\n",
        "#         load_extra_path_config(extra_model_paths)\n",
        "#     else:\n",
        "#         print(\"Could not find the extra_model_paths config file.\")\n",
        "\n",
        "\n",
        "add_comfyui_directory_to_sys_path()\n",
        "# add_extra_model_paths()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3d4edf",
      "metadata": {},
      "source": [
        "Upload all the models that you will need during the generation phase, in this case we will need a model to generate the image starting from the text and another 2-3 models to integrate the generated image into the image of our QR code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d12acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nodes import (\n",
        "    KSampler,\n",
        "    CLIPTextEncode,\n",
        "    ControlNetApplyAdvanced,\n",
        "    VAEDecode,\n",
        "    CheckpointLoaderSimple,\n",
        "    LoadImage,\n",
        "    ControlNetLoader,\n",
        "    NODE_CLASS_MAPPINGS,\n",
        "    EmptyLatentImage,\n",
        "    VAELoader,\n",
        "    SaveImage,\n",
        ")\n",
        "controlnetapplyadvanced = ControlNetApplyAdvanced()\n",
        "ksampler = KSampler()\n",
        "vaedecode = VAEDecode()\n",
        "saveimage = SaveImage()\n",
        "vaeloader = VAELoader()\n",
        "\n",
        "def load_checkpoints(diffuser_model:str = \"dreamshaper_8.safetensors\", controlnet_1:str = \"control_v11f1e_sd15_tile.pth\", controlnet_2: str = \"control_v1p_sd15_brightness.safetensors\", controlnet_3: str = None):\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        checkpointloadersimple = CheckpointLoaderSimple()\n",
        "        checkpointloadersimple_4 = checkpointloadersimple.load_checkpoint(\n",
        "            ckpt_name=diffuser_model\n",
        "        )\n",
        "        emptylatentimage = EmptyLatentImage()\n",
        "        emptylatentimage_5 = emptylatentimage.generate(\n",
        "            width=768, height=768, batch_size=4\n",
        "        )\n",
        "        controlnetloader = ControlNetLoader()\n",
        "        controlnetloader_10 = controlnetloader.load_controlnet(\n",
        "                control_net_name=controlnet_1\n",
        "        )\n",
        "\n",
        "        controlnetloader_11 = controlnetloader.load_controlnet(\n",
        "            control_net_name=controlnet_2\n",
        "        )\n",
        "        controlnetloader_12 = None\n",
        "        if controlnet_3 is not None:\n",
        "            controlnetloader_12 = controlnetloader.load_controlnet(\n",
        "                control_net_name=controlnet_3\n",
        "            )\n",
        "        vaeloader_24 = vaeloader.load_vae(\n",
        "            vae_name=\"vae-ft-mse-840000-ema-pruned.safetensors\"\n",
        "        )\n",
        "        return checkpointloadersimple_4, emptylatentimage_5, controlnetloader_10, controlnetloader_11, controlnetloader_12, vaeloader_24"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2908f2e0",
      "metadata": {},
      "source": [
        "Choose which models to use, pass the same name as the models in the \"checkpoints\" and \"controlnet\" folders. You can decide whether to use two controlnets or three, passing the filenames for controlnet_1 and controlnet_2 or controlnet_1, controlnet_2 and controlnet_3.  By default only two are used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69dc942b",
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpointloadersimple_4, emptylatentimage_5, controlnetloader_10, controlnetloader_11, controlnetloader_12, vaeloader_24 = load_checkpoints()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c18fba",
      "metadata": {},
      "source": [
        "Load the CLIP templates which, given the texts, will allow you to choose which image to generate and which effects to avoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LgpNLB8LUnxE",
      "metadata": {
        "id": "LgpNLB8LUnxE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_text_and_image(prompt_text:str = None, prompt_text_negative:str = None, input_image_path: str = None):\n",
        "  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "  with torch.inference_mode():\n",
        "\n",
        "        cliptextencode = CLIPTextEncode()\n",
        "        cliptextencode_6 = cliptextencode.encode(\n",
        "            text=prompt_text, clip=get_value_at_index(checkpointloadersimple_4, 1)\n",
        "        )\n",
        "\n",
        "        cliptextencode_7 = cliptextencode.encode(\n",
        "            text=prompt_text_negative,\n",
        "            clip=get_value_at_index(checkpointloadersimple_4, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        loadimage = LoadImage()\n",
        "        loadimage_14 = loadimage.load_image(image=input_image_path)\n",
        "\n",
        "\n",
        "        return cliptextencode_6, cliptextencode_7, loadimage_14"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bdd63a8",
      "metadata": {},
      "source": [
        "This is the heart of the generation and allows you to choose the values ​​to attribute to the different controlnets, if you want to play with the parameters and see how the result varies as they vary, you can go to the custom values ​​variable or modify the `start_percent` and `end_percent` values ​​directly in the models. \n",
        "\n",
        "- `strength`: strength of controlnet; 1.0 is full strength, 0.0 is no effect at all.\n",
        "- `start_percent`: sampling step percentage at which controlnet should start to be applied - no matter what start_percent is set on timestep keyframes, they won't take effect until this start_percent is reached.\n",
        "- `stop_percent`: sampling step percentage at which controlnet should stop being applied - no matter what start_percent is set on timestep keyframes, they won't take effect once this end_percent is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gDBPDtOs6bAi",
      "metadata": {
        "id": "gDBPDtOs6bAi"
      },
      "outputs": [],
      "source": [
        "def run_inference(cliptextencode_6, cliptextencode_7, loadimage_14, tile_controlnet_values: float=0.5, brightness_controlnet_values: float = 0.35, depth_controlnet_values: float = 1.0, number_of_examples=3):\n",
        "  with torch.inference_mode():\n",
        "          for _ in range(number_of_examples):\n",
        "            controlnetapplyadvanced_28 = controlnetapplyadvanced.apply_controlnet(\n",
        "                strength=tile_controlnet_values,\n",
        "                start_percent=0.35,\n",
        "                end_percent=0.6,\n",
        "                positive=get_value_at_index(cliptextencode_6, 0),\n",
        "                negative=get_value_at_index(cliptextencode_7, 0),\n",
        "                control_net=get_value_at_index(controlnetloader_10, 0),\n",
        "                image=get_value_at_index(loadimage_14, 0),\n",
        "            )\n",
        "\n",
        "            controlnetapplyadvanced_27 = controlnetapplyadvanced.apply_controlnet(\n",
        "                strength=brightness_controlnet_values,\n",
        "                start_percent=0,\n",
        "                end_percent=1,\n",
        "                positive=get_value_at_index(controlnetapplyadvanced_28, 0),\n",
        "                negative=get_value_at_index(controlnetapplyadvanced_28, 1),\n",
        "                control_net=get_value_at_index(controlnetloader_11, 0),\n",
        "                image=get_value_at_index(loadimage_14, 0),\n",
        "            )\n",
        "            controlnetapplyadvanced_26 = None\n",
        "            if controlnetloader_12 is not None:\n",
        "                controlnetapplyadvanced_26 = controlnetapplyadvanced.apply_controlnet(\n",
        "                    strength=depth_controlnet_values,\n",
        "                    start_percent=0,\n",
        "                    end_percent=0.2,\n",
        "                    positive=get_value_at_index(controlnetapplyadvanced_27, 0),\n",
        "                    negative=get_value_at_index(controlnetapplyadvanced_27, 1),\n",
        "                    control_net=get_value_at_index(controlnetloader_12, 0),\n",
        "                    image=get_value_at_index(loadimage_14, 0),\n",
        "                )\n",
        "\n",
        "          if controlnetapplyadvanced_26 is not None:\n",
        "            ksampler_17 = ksampler.sample(\n",
        "                seed=random.randint(1, 2**64),\n",
        "                steps=20,\n",
        "                cfg=8,\n",
        "                sampler_name=\"euler\",\n",
        "                scheduler=\"normal\",\n",
        "                denoise=1,\n",
        "                model=get_value_at_index(checkpointloadersimple_4, 0),\n",
        "                positive=get_value_at_index(controlnetapplyadvanced_26, 0),\n",
        "                negative=get_value_at_index(controlnetapplyadvanced_26, 1),\n",
        "                latent_image=get_value_at_index(emptylatentimage_5, 0),\n",
        "            )\n",
        "          else:\n",
        "            ksampler_17 = ksampler.sample(\n",
        "                seed=random.randint(1, 2**64),\n",
        "                steps=20,\n",
        "                cfg=8,\n",
        "                sampler_name=\"euler\",\n",
        "                scheduler=\"normal\",\n",
        "                denoise=1,\n",
        "                model=get_value_at_index(checkpointloadersimple_4, 0),\n",
        "                positive=get_value_at_index(controlnetapplyadvanced_27, 0),\n",
        "                negative=get_value_at_index(controlnetapplyadvanced_27, 1),\n",
        "                latent_image=get_value_at_index(emptylatentimage_5, 0),\n",
        "            )\n",
        "\n",
        "            vaedecode_18 = vaedecode.decode(\n",
        "                samples=get_value_at_index(ksampler_17, 0),\n",
        "                vae=get_value_at_index(vaeloader_24, 0),\n",
        "            )\n",
        "\n",
        "            saveimage_29 = saveimage.save_images(\n",
        "                filename_prefix=\"ComfyUI\", images=get_value_at_index(vaedecode_18, 0)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7aab578",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ComfyUI\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OEav7ykGChla",
      "metadata": {
        "id": "OEav7ykGChla"
      },
      "outputs": [],
      "source": [
        "# IF YOU ARE IN COLAB COULD BE QUICKER TO USE THE FOLLOWING PATH\n",
        "#OUTPUT_DIR = \"/content/ComfyUI/output/\"\n",
        "#OUTPUT_DIR_NR = \"/content/ComfyUI/output/non_readable/\"\n",
        "\n",
        "# IF YOU ARE NOT IN COLAB\n",
        "OUTPUT_DIR = \"output/\"\n",
        "OUTPUT_DIR_NR = \"output/non_readable/\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR_NR, exist_ok = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b758672e",
      "metadata": {},
      "source": [
        "Choose the prompt from the first step based on your chosen company, or write a prompt from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48dfc95",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(image_prompt)\n",
        "print(chain_answer)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3PLEiq6PJY8",
      "metadata": {
        "id": "b3PLEiq6PJY8"
      },
      "outputs": [],
      "source": [
        "prompt_text = \"a Deep Lake in the forest\"\n",
        "prompt_text_negative = \"ugly, bad, artifacts\"\n",
        "# must be in the ComfyUI/input/ folder\n",
        "input_image_path = \"activeloop_qr.jpg\"\n",
        "\n",
        "cliptextencode_6, cliptextencode_7, loadimage_14 = load_text_and_image(prompt_text, prompt_text_negative, input_image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b667c246",
      "metadata": {},
      "source": [
        "Run the inference function and check the generated images in the output folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K24n9aoNPcrU",
      "metadata": {
        "id": "K24n9aoNPcrU"
      },
      "outputs": [],
      "source": [
        "run_inference(cliptextencode_6, cliptextencode_7, loadimage_14, tile_controlnet_values=0.5, brightness_controlnet_values=0.35, depth_controlnet_values=1.0, number_of_examples=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9823afaf",
      "metadata": {},
      "source": [
        "Depending on the CheckpointLoaderSimple template loaded (see `load_checkpoints` function), you will get different results with the same prompt.\n",
        "\n",
        "Here, for example, we insert `a cyborg character` as text and the results obtained are the following:\n",
        "\n",
        "### With v1-5-pruned-emaonly.safetensors\n",
        "\n",
        "<img width=\"200\" src=\"images/qrcode_v1.5_1.webp\"> <img width=\"200\" src=\"images/qrcode_v1.5_2.webp\">\n",
        "<img width=\"200\" src=\"images/qrcode_v1.5_3.webp\"> <img width=\"200\" src=\"images/qrcode_v1.5_4.webp\">\n",
        "\n",
        "\n",
        "### With dreamshaper_8.safetensors\n",
        "\n",
        "<img width=\"200\" src=\"images/qrcode_dreamshaper_1.webp\"> <img width=\"200\" src=\"images/qrcode_dreamshaper_2.webp\">\n",
        "<img width=\"200\" src=\"images/qrcode_dreamshaper_3.webp\"> <img width=\"200\" src=\"images/qrcode_dreamshaper_4.webp\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc750f28",
      "metadata": {},
      "source": [
        "Each model has its pros and cons and this depends on what type of generation it was trained for"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e0f36a",
      "metadata": {},
      "source": [
        "### Keep only truly scannable QR codes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nBJphIC-_8sA",
      "metadata": {
        "id": "nBJphIC-_8sA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from qreader import QReader\n",
        "import cv2\n",
        "import shutil\n",
        "from PIL import Image\n",
        "qreader = QReader()\n",
        "\n",
        "\n",
        "def keep_readable_qrcodes(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        path = folder + filename\n",
        "        img = cv2.imread(folder + filename)\n",
        "\n",
        "        if img is not None:\n",
        "          decoded_text = qreader.detect_and_decode(image=img)\n",
        "          if decoded_text:\n",
        "            print(decoded_text)\n",
        "          else:\n",
        "            print(f\"non readable: {filename}\")\n",
        "            shutil.move(path, OUTPUT_DIR_NR + filename)\n",
        "\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yz61lndx_0df",
      "metadata": {
        "id": "yz61lndx_0df"
      },
      "outputs": [],
      "source": [
        "#images = keep_readable_qrcodes(\"/content/ComfyUI/output\")\n",
        "keep_readable_qrcodes(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cb318b",
      "metadata": {},
      "source": [
        "### Apply logo to generated images to make them more intriguing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p_-XZaNZNUVE",
      "metadata": {
        "id": "p_-XZaNZNUVE"
      },
      "outputs": [],
      "source": [
        "generated_image = Image.open(\"output/ComfyUI_00068_.png\")\n",
        "img_with_logo = qr_with_logo(\"activeloop_logo.jpg\", generated_image, \"generated_image_with_logo.jpg\")\n",
        "img_with_logo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f9757f",
      "metadata": {},
      "source": [
        "\n",
        "### Limitations of Our Approach\n",
        "- Overall, the ControlNet model required extensive manual tuning of parameters. There are many methods to control the QR code generation process, but none are entirely reliable. The problem intensifies when you want to account for the input product image as well. To the best of our knowledge, no other publication has found a way to generate them reliably, and we spent the majority of our time experimenting with various setups.\n",
        "\n",
        "- Adding an image to the input might offer more control and bring about various use-cases, but it significantly restricts the possibilities of stable diffusion. This usually only results in changes to the image's style without fitting much of the QR structure. Moreover, we saw greater success with text-to-image compared to image-to-image with logo masks. However, the former wasn't as desirable because we believe logos are essential in product QR codes.\n",
        "\n",
        "- From our examples, it's evident that the generated products don't exactly match the actual products one-to-one. If the goal is to advertise a specific product, even a minor mismatch could be misleading. Nonetheless, we believe that [LORA](https://stable-diffusion-art.com/lora/) models or a different type of preprocessor model could address these issues.\n",
        "\n",
        "- Automated image prompts can sometimes be confusing, drawing focus to unimportant details within the context. This is particularly problematic if we don't have enough relevant textual information to build upon. This presents an opportunity to further use the Deep Lake Vector Store to analyze the image bind embeddings for a better understanding of the content on e-commerce websites.\n",
        "\n",
        "- In our examples, we also encountered issues with faces, as they sometimes didn't appear human. However, this could be easily addressed with further processing. In instances where we want to preserve the face and adjust it to the QR code, there are tools like the [Roop](https://github.com/s0md3v/sd-webui-roop) that can be used for a detailed face replacement.\n",
        "\n",
        "\n",
        "### Conclusion: Scalable Prompt Generation Achieved, QR Code Generation Remains Unreliable\n",
        "Deep Lake combined with LangChain can significantly reduce the costs of analyzing the contents of a website to provide image descriptions in a scalable way. Thanks to the Deep Lake Vector Store, we can save a large number of documents and images along with their embeddings. This allows us to iteratively adjust the image prompts and efficiently filter based on embedding similarities. However, it is very difficult to find the ControlNet sweet spot of QR readability and \"cool\" design. Taking into account all of the limitations we've discussed, we believe that there needs to be more experimenting with ControlNet, in order to generated product QR codes that are reliable and applicable for real-world businesses.\n",
        "\n",
        "I hope that you find this useful and already have many ideas on how to further build on this. Thank you for reading and I wish you a great day and see you in the next one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f35af65",
      "metadata": {},
      "source": [
        "## FAQs\n",
        "<h2 id=\"faq\">What is prompt engineering?</h2>\n",
        "     Prompt engineering is the practice of carefully crafting inputs (prompts) to be given to AI models, particularly language models, in order to elicit the desired output. It involves understanding how the model interprets inputs and using that knowledge to achieve more accurate, relevant, or creative responses.\n",
        "\n",
        "<h2 id=\"faq\">What is Stable Diffusion?</h2>\n",
        "     Stable Diffusion refers to a specific generative artificial intelligence model designed for text-to-image synthesis. This model has the capability to generate photorealistic images based on textual input. It empowers users to create stunning artwork quickly and autonomously. Additionally, besides images, Stable Diffusion can also be used for image-to-image generation or to create videos and animations. It was originally launched in 2022.\n",
        "\n",
        "<h2 id=\"faq\">Is Stable Diffusion free to use?</h2>\n",
        "     Stable Diffusion is an open-source project, which means it is freely available for anyone to use. You can access and use Stable Diffusion without any cost, subject to the terms of its open-source license.\n",
        "\n",
        "<h2 id=\"faq\">What is ControlNet?</h2>\n",
        "     ControlNet is an innovative neural network architecture that integrates extra conditions to manage the control of diffusion models. These techniques include edge and line detection, human poses, image segmentation, depth maps, image styles, or simple user scribbles, allowing for conditioned output images.\n",
        "\n",
        "<h2 id=\"faq\">What is AUTOMATIC1111?</h2>\n",
        "     AUTOMATIC1111 is a robust web-based user interface (WebUI) tailored for Stable Diffusion, an AI model for text-to-image generation. It provides an intuitive platform for creating remarkable images from textual prompts.\n",
        "\n",
        "<h2 id=\"faq\">How to use ComfyUI?</h2> \n",
        "     ComfyUI is a powerful and modular stable diffusion GUI. To use it, you need to clone the ComfyUI repository, install its requirements, and run the main.py file. It allows for the creation of schemas from the GUI and transforms these schemas into code for image generation tasks.\n",
        "\n",
        "<h2 id=\"faq\">What is QR code and how it works?</h2>\n",
        "     A QR code, or Quick Response code, is a two-dimensional barcode that stores data. It works by encoding information in black squares arranged on a white grid. When scanned by a QR code reader or smartphone camera, the encoded data is decoded and can trigger actions such as opening a website or displaying text.\n",
        "\n",
        "<h2 id=\"faq\">How to make artistic QR codes?</h2>\n",
        "     Making artistic QR codes involves incorporating design elements, colors, and sometimes logos into the QR code without compromising its scan-ability. This can be achieved through specialized software or online tools that allow for the customization of QR codes while ensuring they remain functional.\n",
        "\n",
        "<h2 id=\"faq\">What is image synthesis?</h2>\n",
        "     Image synthesis is the process of generating new images from textual descriptions, existing images, or a combination of both, using artificial intelligence and machine learning models. It involves creating visually coherent and contextually relevant images based on the input provided.\n",
        "     \n",
        "<h2 id=\"faq\">What are LoRA models?</h2>\n",
        "     LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. In the context of AI and machine learning, particularly concerning stable diffusion models, LoRA models are mentioned as potentially useful for addressing issues in generating product QR codes that match actual products more closely."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45dc1839",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
